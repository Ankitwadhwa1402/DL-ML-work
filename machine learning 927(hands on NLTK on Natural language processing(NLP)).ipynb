{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USED to analyse the feedback or any text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as it is case sensitive so evey word in python have different meaning so we use lower casing to make these words same\n",
    "#like THe,The,the,thE all are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "str='Welcome To THe world of MY pYThon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'welcome to the world of my python'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus means folder having different types of files like texts\n",
    "from nltk.corpus import gutenberg as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.fileids()  #files present in gt corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt.words('austen-emma.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.sents('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7752"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt.sents('austen-emma.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887071"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt.raw('austen-emma.txt') ) #no of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female.txt', 'male.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.fileids()  #having gender names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stopwords(useless words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#like- am,are,of,to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids() # langauges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english') # stopwords in english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_word=gt.words('austen-emma.txt')\n",
    "actual_word[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'CHAPTER', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'rich', ',', 'comfortable', 'home', 'happy', 'disposition', ',', 'seemed', 'unite', 'best', 'blessings', 'existence', ';', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'world', 'little', 'distress', 'vex', '.', 'youngest', 'two', 'daughters', 'affectionate', ',', 'indulgent', 'father', ';', ',', 'consequence', 'sister', \"'\", 'marriage', ',', 'mistress', 'house', 'early', 'period', '.', 'mother', 'died', 'long', 'ago', 'indistinct', 'remembrance', 'caresses', ';', 'place', 'supplied', 'excellent', 'woman', 'governess', ',', 'fallen', 'little', 'short', 'mother', 'affection', '.', 'Sixteen', 'years', 'Miss', 'Taylor', 'Mr', '.', 'Woodhouse', \"'\", 'family', ',', 'less', 'governess', 'friend', ',', 'fond', 'daughters', ',', 'particularly', 'Emma', '.', '_them_', 'intimacy', 'sisters', '.', 'Even', 'Miss', 'Taylor', 'ceased', 'hold', 'nominal', 'office', 'governess', ',', 'mildness', 'temper', 'hardly', 'allowed', 'impose', 'restraint', ';', 'shadow', 'authority', 'long', 'passed', 'away', ',', 'living', 'together', 'friend', 'friend', 'mutually', 'attached', ',', 'Emma', 'liked', ';', 'highly', 'esteeming', 'Miss', 'Taylor', \"'\", 'judgment', ',', 'directed', 'chiefly', '.', 'real', 'evils', ',', 'indeed', ',', 'Emma', \"'\", 'situation', 'power', 'rather', 'much', 'way', ',', 'disposition', 'think', 'little', 'well', ';', 'disadvantages', 'threatened', 'alloy', 'many', 'enjoyments', '.', 'danger', ',', 'however', ',', 'present', 'unperceived', ',', 'means', 'rank', 'misfortunes', '.', 'Sorrow', 'came', '--', 'gentle', 'sorrow', '--', 'shape', 'disagreeable', 'consciousness', '.--', 'Miss', 'Taylor', 'married', '.', 'Miss', 'Taylor', \"'\", 'loss', 'first', 'brought', 'grief', '.', 'wedding', '-', 'day', 'beloved', 'friend', 'Emma', 'first', 'sat', 'mournful', 'thought', 'continuance', '.', 'wedding', ',', 'bride', '-', 'people', 'gone', ',', 'father', 'left', 'dine', 'together', ',', 'prospect', 'third', 'cheer', 'long', 'evening', '.', 'father', 'composed', 'sleep', 'dinner', ',', 'usual', ',', 'sit', 'think', 'lost', '.', 'event', 'every', 'promise', 'happiness', 'friend', '.', 'Mr', '.', 'Weston', 'man', 'unexceptionable', 'character', ',', 'easy', 'fortune', ',', 'suitable', 'age', ',', 'pleasant', 'manners', ';', 'satisfaction', 'considering', 'self', '-', 'denying', ',', 'generous', 'friendship', 'always', 'wished', 'promoted', 'match', ';', 'black', 'morning', \"'\", 'work', '.', 'want', 'Miss', 'Taylor', 'would', 'felt', 'every', 'hour', 'every', 'day', '.', 'recalled', 'past', 'kindness', '--', 'kindness', ',', 'affection', 'sixteen', 'years', '--', 'taught', 'played', 'five', 'years', 'old', '--', 'devoted', 'powers', 'attach', 'amuse', 'health', '--', 'nursed', 'various', 'illnesses', 'childhood', '.', 'large', 'debt', 'gratitude', 'owing', ';', 'intercourse', 'last', 'seven', 'years', ',', 'equal', 'footing', 'perfect', 'unreserve', 'soon', 'followed', 'Isabella', \"'\", 'marriage', ',', 'left', ',', 'yet', 'dearer', ',', 'tenderer', 'recollection', '.', 'friend', 'companion', 'possessed', ':', 'intelligent', ',', 'well', '-', 'informed', ',', 'useful', ',', 'gentle', ',', 'knowing', 'ways', 'family', ',', 'interested', 'concerns', ',', 'peculiarly', 'interested', ',', 'every', 'pleasure', ',', 'every', 'scheme', '--', 'one', 'could', 'speak', 'every', 'thought', 'arose', ',', 'affection', 'could', 'never', 'find', 'fault', '.', 'bear', 'change', '?--', 'true', 'friend', 'going', 'half', 'mile', ';', 'Emma', 'aware', 'great', 'must', 'difference', 'Mrs', '.', 'Weston', ',', 'half', 'mile', ',', 'Miss', 'Taylor', 'house', ';', 'advantages', ',', 'natural', 'domestic', ',', 'great', 'danger', 'suffering', 'intellectual', 'solitude', '.', 'dearly', 'loved', 'father', ',', 'companion', '.', 'could', 'meet', 'conversation', ',', 'rational', 'playful', '.', 'evil', 'actual', 'disparity', 'ages', '(', 'Mr', '.', 'Woodhouse', 'married', 'early', ')', 'much', 'increased', 'constitution', 'habits', ';', 'valetudinarian', 'life', ',', 'without', 'activity', 'mind', 'body', ',', 'much', 'older', 'man', 'ways', 'years', ';', 'though', 'everywhere', 'beloved', 'friendliness', 'heart', 'amiable', 'temper', ',', 'talents', 'could', 'recommended', 'time', '.', 'sister', ',', 'though', 'comparatively', 'little', 'removed', 'matrimony', ',', 'settled', 'London', ',', 'sixteen', 'miles', ',', 'much', 'beyond', 'daily', 'reach', ';', 'many', 'long', 'October', 'November', 'evening', 'must', 'struggled', 'Hartfield', ',', 'Christmas', 'brought', 'next', 'visit', 'Isabella', 'husband', ',', 'little', 'children', ',', 'fill', 'house', ',', 'give', 'pleasant', 'society', '.', 'Highbury', ',', 'large', 'populous', 'village', ',', 'almost', 'amounting', 'town', ',', 'Hartfield', ',', 'spite', 'separate', 'lawn', ',', 'shrubberies', ',', 'name', ',']\n"
     ]
    }
   ],
   "source": [
    "#remove the stopwords from a file\n",
    "\n",
    "def filter_content(text):\n",
    "    fil_content=[w for w in text if w.lower() not in stopwords.words('english')]\n",
    "    return fil_content\n",
    "\n",
    "\n",
    "\n",
    "print(filter_content(actual_word[:1000]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'almost', 'allowed', 'first', 'reach', 'misfortunes', 'judgment', 'VOLUME', 'chiefly', 'situation', 'played', 'excellent', 'indistinct', 'husband', 'taught', 'mistress', 'great', 'sister', 'attach', 'many', 'seemed', 'useful', 'directed', 'gratitude', 'footing', 'settled', 'removed', 'felt', 'mind', 'evil', 'debt', 'Mr', 'arose', 'name', 'passed', 'generous', 'would', 'people', 'restraint', 'daily', 'matrimony', 'peculiarly', 'cheer', 'came', 'speak', 'CHAPTER', 'difference', 'two', 'playful', 'knowing', 'Mrs', 'easy', 'dearer', 'conversation', 'father', 'temper', 'lost', 'shadow', 'friendliness', 'woman', 'without', 'self', 'sixteen', 'older', 'tenderer', '(', ']', 'age', 'suffering', 'denying', 'gone', 'died', '--', 'must', 'event', 'alloy', 'fill', 'mutually', 'shape', 'give', '1816', 'thought', 'youngest', 'possessed', 'loss', 'man', 'illnesses', 'indulgent', 'always', 'promise', 'disparity', '.--', 'amounting', 'village', 'world', 'intimacy', 'caresses', 'nearly', 'intercourse', 'November', 'sleep', 'present', 'Weston', 'disagreeable', 'interested', 'scheme', 'consciousness', 'morning', 'hour', 'sit', 'equal', 'black', 'married', 'Highbury', 'bear', 'ceased', 'society', 'constitution', 'family', 'particularly', 'clever', 'blessings', 'promoted', 'ways', 'lawn', 'rational', 'affectionate', 'fallen', 'dine', 'amuse', 'handsome', '.', 'pleasant', 'though', 'living', 'power', 'happy', 'sat', 'Jane', 'composed', 'visit', 'unperceived', 'danger', 'populous', 'well', ')', 'liked', 'impose', 'going', 'soon', 'home', 'beloved', 'character', 'consequence', 'remembrance', 'prospect', 'Emma', 'could', 'every', 'governess', 'however', 'twenty', 'recommended', 'owing', '-', 'sisters', 'Christmas', 'companion', 'together', 'place', 'seven', 'five', 'large', 'mournful', 'concerns', 'intelligent', 'miles', 'old', 'unite', 'early', 'aware', 'years', 'October', 'think', '_them_', 'powers', 'Hartfield', 'long', 'happiness', 'real', 'various', 'fault', 'life', 'comfortable', 'dinner', 'everywhere', 'gentle', 'esteeming', ';', 'intellectual', 'satisfaction', 'find', 'best', 'friendship', 'short', 'shrubberies', 'usual', 'unexceptionable', 'sorrow', 'ago', 'natural', 'amiable', 'dearly', 'Woodhouse', 'town', 'valetudinarian', 'advantages', \"'\", 'fortune', 'separate', 'one', 'increased', 'indeed', 'never', 'Sorrow', 'followed', 'actual', 'pleasure', 'vex', 'change', 'evils', ',', 'little', 'Sixteen', 'devoted', 'Taylor', 'attached', 'means', 'struggled', 'activity', 'daughters', 'wished', 'half', 'nursed', 'suitable', 'left', 'childhood', 'perfect', 'supplied', 'recollection', 'want', 'fond', 'much', 'day', 'yet', 'habits', 'bride', 'Miss', 'solitude', 'third', 'loved', 'health', 'brought', 'Even', 'disadvantages', 'beyond', 'London', 'heart', 'past', '?--', ':', 'house', 'grief', 'recalled', 'way', 'true', 'meet', '[', 'evening', 'next', 'kindness', 'authority', 'mother', 'hardly', 'lived', 'work', 'informed', 'threatened', 'manners', 'talents', 'rich', 'disposition', 'rather', 'highly', 'continuance', 'nominal', 'hold', 'office', 'match', 'period', 'wedding', 'ages', 'marriage', 'last', 'existence', 'enjoyments', 'considering', 'spite', 'rank', 'time', 'Isabella', 'unreserve', 'away', 'domestic', 'body', 'mile', 'mildness', 'less', 'distress', 'affection', 'children', 'friend', 'comparatively', 'Austen'}\n"
     ]
    }
   ],
   "source": [
    "#for unique words only\n",
    "print(set(filter_content(actual_word[:1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.7"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#content fraction means usable text percentage after removing stopwords\n",
    "con_frac=len(filter_content(actual_word[:1000]))/len(actual_word[:1000])\n",
    "con_frac=con_frac*100\n",
    "con_frac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Game of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, a series of fantasy novels by George R. R. Martin, the first of which is A Game of Thrones. The show was shot in the United Kingdom, Canada, Croatia, Iceland, Malta, Morocco, and Spain. It premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nltk convert string into nltk.text\n",
    "\n",
    "text=nltk.Text(text.split())  #.split is used to separate which are separated by spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterc=filter_content(text)\n",
    "fraction=len(filterc)/len(text)\n",
    "fraction=fraction*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Game', 'Thrones', 'American', 'fantasy', 'drama', 'television', 'series', 'created', 'David', 'Benioff', 'D.', 'B.', 'Weiss', 'HBO.', 'adaptation', 'Song', 'Ice', 'Fire,', 'series', 'fantasy', 'novels', 'George', 'R.', 'R.', 'Martin,', 'first', 'Game', 'Thrones.', 'show', 'shot', 'United', 'Kingdom,', 'Canada,', 'Croatia,', 'Iceland,', 'Malta,', 'Morocco,', 'Spain.', 'premiered', 'HBO', 'United', 'States', 'April', '17,', '2011,', 'concluded', 'May', '19,', '2019,', '73', 'episodes', 'broadcast', 'eight', 'seasons.']\n",
      "60.0\n"
     ]
    }
   ],
   "source": [
    "print(filterc)\n",
    "print(fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokensiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaking of text into words or sentence\n",
    "#mostly word tokenisation preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "#sentence separated by fullsto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HEllo SF python.', 'This is NLTK.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str=\"HEllo SF python. This is NLTK.\"\n",
    "sent_tokenize(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Game of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO.',\n",
       " 'It is an adaptation of A Song of Ice and Fire, a series of fantasy novels by George R. R. Martin, the first of which is A Game of Thrones.',\n",
       " 'The show was shot in the United Kingdom, Canada, Croatia, Iceland, Malta, Morocco, and Spain.',\n",
       " 'It premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1=\"Game of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, a series of fantasy novels by George R. R. Martin, the first of which is A Game of Thrones. The show was shot in the United Kingdom, Canada, Croatia, Iceland, Malta, Morocco, and Spain. It premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.\"\n",
    "sent_tokenize(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HEllo', 'SF', 'python', '.', 'This', 'is', 'NLTK', '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(set(word_tokenize('This is NLTK.'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(set(word_tokenize('This is NLTK. This is python'))))  #counts unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter text: wwnndsndlsn sck cwdww  w k \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wwnndsndlsn', 'sck', 'cwdww', 'w', 'k']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on user input\n",
    "cap=input('enter text: ')\n",
    "word_tokenize(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(cap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove punctuation(@,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"I am so excited for the show tonight!!. What? Do you want me to cook?. Did you see the report @today\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'so', 'excited', 'for', 'the', 'show', 'tonight', 'what', 'do', 'you', 'want', 'me', 'to', 'cook', 'did', 'you', 'see', 'the', 'report', 'today']\n"
     ]
    }
   ],
   "source": [
    "#isalpha means not a punctuation\n",
    "words=[w.lower() for w in words if w.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=\"Game of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for using stopwords covert text into ntlk.text or in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "['game', 'thrones', 'american', 'fantasy', 'drama', 'television', 'series', 'created', 'david', 'benioff', 'd.', 'b.', 'weiss', 'hbo', '.']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "words=word_tokenize(t1)\n",
    "print(len(words))\n",
    "filtered=[w.lower() for w in words if w not in stopwords.words(\"english\")]\n",
    "print(filtered)\n",
    "print(len(filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play,playing,plays,played to make these all are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=nltk.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "play\n",
      "cat\n",
      "car\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('playing'))\n",
    "print(ps.stem('plays'))\n",
    "print(ps.stem('cats'))\n",
    "print(ps.stem('cars'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program\n",
      "program\n",
      "program\n",
      "program\n",
      "program\n"
     ]
    }
   ],
   "source": [
    "words=['program','programs','programer','programing','programers']\n",
    "\n",
    "for i in words:\n",
    "    print(ps.stem(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text='In machine learning, support-vector machines (SVMs, also support-vector networks[1]) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Vapnik et al., 1997), SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "machin\n",
      "learn\n",
      ",\n",
      "support-vector\n",
      "machin\n",
      "(\n",
      "svm\n",
      ",\n",
      "also\n",
      "support-vector\n",
      "network\n",
      "[\n",
      "1\n",
      "]\n",
      ")\n",
      "are\n",
      "supervis\n",
      "learn\n",
      "model\n",
      "with\n",
      "associ\n",
      "learn\n",
      "algorithm\n",
      "that\n",
      "analyz\n",
      "data\n",
      "for\n",
      "classif\n",
      "and\n",
      "regress\n",
      "analysi\n",
      ".\n",
      "develop\n",
      "at\n",
      "at\n",
      "&\n",
      "t\n",
      "bell\n",
      "laboratori\n",
      "by\n",
      "vladimir\n",
      "vapnik\n",
      "with\n",
      "colleagu\n",
      "(\n",
      "boser\n",
      "et\n",
      "al.\n",
      ",\n",
      "1992\n",
      ",\n",
      "guyon\n",
      "et\n",
      "al.\n",
      ",\n",
      "1993\n",
      ",\n",
      "vapnik\n",
      "et\n",
      "al.\n",
      ",\n",
      "1997\n",
      ")\n",
      ",\n",
      "svm\n",
      "are\n",
      "one\n",
      "of\n",
      "the\n",
      "most\n",
      "robust\n",
      "predict\n",
      "method\n",
      ",\n",
      "be\n",
      "base\n",
      "on\n",
      "statist\n",
      "learn\n",
      "framework\n",
      "or\n",
      "vc\n",
      "theori\n",
      "propos\n",
      "by\n",
      "vapnik\n",
      "(\n",
      "1982\n",
      ",\n",
      "1995\n",
      ")\n",
      "and\n",
      "chervonenki\n",
      "(\n",
      "1974\n",
      ")\n",
      ".\n",
      "given\n",
      "a\n",
      "set\n",
      "of\n",
      "train\n",
      "exampl\n",
      ",\n",
      "each\n",
      "mark\n",
      "as\n",
      "belong\n",
      "to\n",
      "one\n",
      "of\n",
      "two\n",
      "categori\n",
      ",\n",
      "an\n",
      "svm\n",
      "train\n",
      "algorithm\n",
      "build\n",
      "a\n",
      "model\n",
      "that\n",
      "assign\n",
      "new\n",
      "exampl\n",
      "to\n",
      "one\n",
      "categori\n",
      "or\n",
      "the\n",
      "other\n",
      ",\n",
      "make\n",
      "it\n",
      "a\n",
      "non-probabilist\n",
      "binari\n",
      "linear\n",
      "classifi\n",
      "(\n",
      "although\n",
      "method\n",
      "such\n",
      "as\n",
      "platt\n",
      "scale\n",
      "exist\n",
      "to\n",
      "use\n",
      "svm\n",
      "in\n",
      "a\n",
      "probabilist\n",
      "classif\n",
      "set\n",
      ")\n",
      ".\n",
      "svm\n",
      "map\n",
      "train\n",
      "exampl\n",
      "to\n",
      "point\n",
      "in\n",
      "space\n",
      "so\n",
      "as\n",
      "to\n",
      "maximis\n",
      "the\n",
      "width\n",
      "of\n",
      "the\n",
      "gap\n",
      "between\n",
      "the\n",
      "two\n",
      "categori\n",
      ".\n",
      "new\n",
      "exampl\n",
      "are\n",
      "then\n",
      "map\n",
      "into\n",
      "that\n",
      "same\n",
      "space\n",
      "and\n",
      "predict\n",
      "to\n",
      "belong\n",
      "to\n",
      "a\n",
      "categori\n",
      "base\n",
      "on\n",
      "which\n",
      "side\n",
      "of\n",
      "the\n",
      "gap\n",
      "they\n",
      "fall\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "word_tokens=nltk.word_tokenize(new_text)\n",
    "for i in word_tokens:\n",
    "    print(ps.stem(i))\n",
    "    \n",
    "#some words are not actuallike categorical is categori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strange\n",
      "play\n",
      "import\n",
      "categ\n"
     ]
    }
   ],
   "source": [
    "print((ls.stem('strange')))\n",
    "print((ls.stem('plays')))\n",
    "print((ls.stem('important')))\n",
    "print((ls.stem('categorical')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play : play\n",
      "eat : eat\n",
      "play : play\n",
      "write : writ\n"
     ]
    }
   ],
   "source": [
    "#PS vs LS\n",
    "\n",
    "w=['playing','eating','played','writing']\n",
    "\n",
    "for i in w:\n",
    "    print(ps.stem(i),end=\" : \")\n",
    "    print(ls.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the playing into play\n",
    "#it removes the ing(affixes) from troubling and remaining word compares in dictionary if it has meaning \n",
    "#then remove otherwise give without removing\n",
    "#but in stemming it removes e from trouble but lemm not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase\n",
      "stone\n",
      "speaking\n",
      "bedroom\n",
      "joke\n",
      "strange\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('increases'))\n",
    "print(lemmatizer.lemmatize('stones'))\n",
    "print(lemmatizer.lemmatize('speaking'))\n",
    "print(lemmatizer.lemmatize('bedroom'))\n",
    "print(lemmatizer.lemmatize('jokes'))\n",
    "print(lemmatizer.lemmatize('strange'))\n",
    "\n",
    "#no word is cutted like strange is not strang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.5.0.tar.gz (622 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Using legacy 'setup.py install' for autocorrect, since package 'wheel' is not installed.\n",
      "Installing collected packages: autocorrect\n",
      "    Running setup.py install for autocorrect: started\n",
      "    Running setup.py install for autocorrect: finished with status 'done'\n",
      "Successfully installed autocorrect-2.5.0\n"
     ]
    }
   ],
   "source": [
    "pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'iss', 'a', 'world', 'of', 'python', 'lets', 'give', 'beautiful', 'hope']\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "spell=Speller(lang='en')\n",
    "tex='This iss a wrld of pythn lets gve beatiful hope'\n",
    "tex1=nltk.word_tokenize(tex)\n",
    "spells=[spell(w) for w in tex1]\n",
    "print(spells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return ''.join(c for c in text if c not in punctuation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how Are you doing\n"
     ]
    }
   ],
   "source": [
    "s='hello! how Are you doing?.'\n",
    "\n",
    "print(remove_punctuation(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am so excited for the show tonight What Do you want me to cook Did you see the report today'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=\"I am so excited for the show tonight!!. What? Do you want me to cook?. Did you see the report @today\"\n",
    "remove_punctuation(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To remove no. from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are  people right next to me at pm. '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#isdigit()  or isnumeric()\n",
    "\n",
    "text=\"There are 200 people right next to me at 2pm. \"\n",
    "\n",
    "ans=''.join(c for c in text if not c.isdigit())\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open the dile and read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('pride.txt',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, a series of fantasy novels by George R. R. Martin, the first of which is A Game of Thrones. The show was shot in the United Kingdom, Canada, Croatia, Iceland, Malta, Morocco, and Spain. It premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.\n",
      "\n",
      "Set on the fictional continents of Westeros and Essos, Game of Thrones has a large ensemble cast and follows several story arcs throughout the course of the show. The first major arc concerns the Iron Throne of the Seven Kingdoms of Westeros through a web of political conflicts among the noble families either vying to claim the throne or fighting for independence from whoever sits on it. A second focuses on the last descendant of the realm's deposed ruling dynasty, who has been exiled to Essos and is plotting a return and reclaim the throne. The third follows the Night's Watch, a military order defending the realm against threats from beyond Westeros's northern border.\n",
      "\n",
      "Game of Thrones attracted a record viewership on HBO and has a broad, active, and international fan base. Critics have praised the series for its acting, complex characters, story, scope, and production values, although its frequent use of nudity and violence (including sexual violence) has been subject to criticism. The final season received significant critical backlash for its reduced length and creative decisions, with many considering it a disappointing conclusion. The series received 59 Primetime Emmy Awards, the most by a drama series, including Outstanding Drama Series in 2015, 2016, 2018 and 2019. Its other awards and nominations include three Hugo Awards for Best Dramatic Presentation, a Peabody Award, and five nominations for the Golden Globe Award for Best Television Series – Drama. Many critics and publications have named the show as one of the best television series of all time.\n"
     ]
    }
   ],
   "source": [
    "text=f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
